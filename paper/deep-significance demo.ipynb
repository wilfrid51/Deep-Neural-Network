{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep-significance Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will demonstrate some of the functionalities in the deep-significance package using the Cart Pole problem (Barto et al. 1983) as implemented in OpenAI gym. \n",
    "\n",
    "Since this is a demo, we will use an extremely simple approach to tackling reinforcement learning problems with neural networks, namely *Deep Q-networks* (Mnih et al., 2015). Back in 2015, Deep Q-networks where the first approach to obtain competitive scores on many Atari games. In this demo, we will specificly use the package to determine the effect of replay memory on the model. \n",
    "\n",
    "Deep Q-Learning tries to approximate the optimal action-value function defined as \n",
    "\n",
    "\\begin{equation*}\n",
    "    Q^*(s, a) = \\max_\\pi \\mathbb{E}\\big[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\ldots \\big| s_t = s, a_t = a, \\pi \\big]\n",
    "\\end{equation*}\n",
    "\n",
    "The definition above reads as follow: The optimal action-value function is the policy $\\pi$ that maximizes the future reward $r_t$ at a state $s_t$ by performing an action $a_t$, with subsequent rewards being increasingly discounted by a factor $\\gamma$. The model weights are updated using the following $l_2$ loss:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s^\\prime) \\sim U(\\text{Buffer})}\\bigg[\\Big(r + \\max_{a^\\prime} Q(s^\\prime, a^\\prime; \\theta^\\text{target}) - Q(s, a; \\theta)\\Big)^2\\bigg]\n",
    "\\end{equation*}\n",
    "\n",
    "Two aspects of this loss function are especially noteworthy: First of all, since we do not know the true value of the $Q$-function in most cases, the predicted value $Q(s, a; \\theta)$ is compared against the reward plus outcome of the greedy action chosen by a *target* network: To avoid having to ``hit a moving target'' (Van Hasselt et al., 2018), the target network is only updated every couple of training steps by copying the main networks parameters. Secondly, the state, action and reward used to compute the loss are not the ones just observed by the model, but instead are uniformly sampled from a *replay buffer*, a sort of memory that past experiences gets added to during training.\n",
    "\n",
    "For that purpose, let us first define the environment along with some project requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STD\n",
    "import random\n",
    "\n",
    "# EXT\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import package functions\n",
    "# To use deepsig in your project, simply use pip install deepsig\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from deepsig import aso, multi_aso, aso_uncertainty_reduction, bootstrap_power_analysis, bootstrap_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "SEED = 42\n",
    "\n",
    "# Set hyperparameters\n",
    "NUM_EPISODES = 100\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 128\n",
    "DISCOUNT_FACTOR = 0.8\n",
    "LEARN_RATE = 1e-3\n",
    "NUM_HIDDEN = 256\n",
    "MEMORY_SIZE = 10000\n",
    "SHOW_AGENT = False  # Set this to true if you want to see the agent learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1068e4570>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "\n",
    "# Seed for replicability\n",
    "env.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a super simple Deep Q-network and replay memory class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_out, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(n_in, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        if self.capacity == len(self.memory):\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    with torch.no_grad():\n",
    "        action = model(torch.Tensor(state))\n",
    "        return torch.argmax(action).item() if random.random() > epsilon else random.choice([0,1])\n",
    "\n",
    "def get_epsilon(it):\n",
    "    return 0.05 if it >= 1000 else - 0.00095 * it + 1\n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor, target_net):\n",
    "\n",
    "    targets = reward + (target_net(next_state).max(1)[0] * discount_factor) * (1 - done.float())\n",
    "    \n",
    "    return targets.unsqueeze(1)\n",
    "\n",
    "def compute_q_val(model, state, action):\n",
    "    q_val = model(state)\n",
    "    q_val = q_val.gather(1, action.unsqueeze(1).view(-1, 1))\n",
    "    return q_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, memory, optimizer, batch_size, discount_factor, target_net):\n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "\n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "\n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    action = action.squeeze()\n",
    "\n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "\n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor, target_net)\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate, target_net,\n",
    "                 target_update_freq, max_steps, show_agent):\n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        cum_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            if show_agent:\n",
    "                env.render()\n",
    "            \n",
    "            steps += 1\n",
    "\n",
    "            eps = get_epsilon(global_steps)\n",
    "\n",
    "            action = select_action(model, state, eps)\n",
    "\n",
    "            if steps % target_update_freq == 0:\n",
    "                target_net.load_state_dict(model.state_dict())\n",
    "\n",
    "            train(model, memory, optimizer, batch_size, discount_factor, target_net)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            cum_reward += reward\n",
    "\n",
    "            memory.push((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "\n",
    "            if steps >= max_steps:\n",
    "                done = True\n",
    "\n",
    "        global_steps += steps\n",
    "        episode_rewards.append(cum_reward)\n",
    "    \n",
    "    if show_agent:\n",
    "        env.close()\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "def run_dqn(env, num_episodes, memory_size, num_hidden, batch_size, discount_factor, learn_rate, target_update_freq,\n",
    "            max_steps, show_agent):\n",
    "    memory = ReplayMemory(memory_size)\n",
    "    n_out = env.action_space.n\n",
    "\n",
    "    n_in = len(env.observation_space.low)\n",
    "    model = QNetwork(n_in, n_out, num_hidden)\n",
    "    target_net = QNetwork(n_in, n_out, num_hidden)\n",
    "\n",
    "    cum_reward = run_episodes(\n",
    "        train=train, model=model, memory=memory, env=env, num_episodes=num_episodes, batch_size=batch_size,\n",
    "        discount_factor=discount_factor, learn_rate=learn_rate, target_net=target_net,\n",
    "        target_update_freq=target_update_freq, max_steps=max_steps, show_agent=show_agent\n",
    "    )\n",
    "    return cum_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the main code ready, we would now like to perform some experiments. Namely, we would like to find out what kind of effect the number of steps to update the target network has on the cumulative rewards. A first way to do this is to run one agent for two different setting (namely 10 and 20) and compare the distributions over rewards obtained during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244.0, 175.0, 343.0, 244.0, 371.0, 230.0, 251.0, 284.0, 268.0, 267.0, 231.0, 60.0, 173.0, 303.0, 310.0, 224.0, 500.0, 465.0, 249.0, 308.0]\n",
      "[194.0, 250.0, 213.0, 162.0, 203.0, 337.0, 76.0, 170.0, 208.0, 207.0, 290.0, 212.0, 232.0, 217.0, 175.0, 207.0, 179.0, 209.0, 212.0, 189.0]\n"
     ]
    }
   ],
   "source": [
    "rewards_freq_10 = run_dqn(\n",
    "    env, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_episodes=NUM_EPISODES, \n",
    "    memory_size=MEMORY_SIZE, \n",
    "    num_hidden=NUM_HIDDEN, \n",
    "    discount_factor=DISCOUNT_FACTOR, \n",
    "    learn_rate=LEARN_RATE, \n",
    "    target_update_freq=10, \n",
    "    max_steps=MAX_STEPS,\n",
    "    show_agent=SHOW_AGENT\n",
    ")\n",
    "\n",
    "rewards_freq_20 = run_dqn(\n",
    "    env, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_episodes=NUM_EPISODES, \n",
    "    memory_size=MEMORY_SIZE, \n",
    "    num_hidden=NUM_HIDDEN, \n",
    "    discount_factor=DISCOUNT_FACTOR, \n",
    "    learn_rate=LEARN_RATE, \n",
    "    target_update_freq=10, \n",
    "    max_steps=MAX_STEPS,\n",
    "    show_agent=SHOW_AGENT\n",
    ")\n",
    "\n",
    "# Print the last 20 rewards for both\n",
    "print(rewards_freq_10[-20:])\n",
    "print(rewards_freq_20[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks relatively similar, so which approach was more successful? We can try to answer this question using the Almost Stochastic Order test (ASO). Roughly, it works by comparing the overlap of the two empricial cumulative distribution functions of scores and checking for their overlap - if one approach approach is yielding consistently higher rewards compared to the other one, they should not overlap (and the test score should be close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 999/1000 [00:05<00:00, 187.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1315818429367487"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aso(rewards_freq_10, rewards_freq_20, num_jobs=4, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test scores is quite low, this gives us an indication that waiting 20 steps to update the target network might be beneficial! Nevertheless, this comes with a caveat - we only checked one model run per value, and neural networks are infamous for being sensitive to their random initialization. Therefore, instead of comparing the reward distributions of two single models per run, let us compare the **distribution over average rewards over multiple runs**. We start by doing 5 runs each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run #1...\n",
      "Performing run #2...\n",
      "Performing run #3...\n",
      "Performing run #4...\n",
      "Performing run #5...\n",
      "[152.92, 107.4, 135.69, 138.38, 170.26]\n",
      "[151.41, 129.96, 146.2, 96.06, 159.39]\n"
     ]
    }
   ],
   "source": [
    "reward_dist_freq_10, reward_dist_freq_20 = [], []\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Performing run #{i+1}...\")\n",
    "    reward_dist_freq_10.append(\n",
    "        np.mean(run_dqn(\n",
    "            env, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_episodes=NUM_EPISODES, \n",
    "            memory_size=MEMORY_SIZE, \n",
    "            num_hidden=NUM_HIDDEN, \n",
    "            discount_factor=DISCOUNT_FACTOR, \n",
    "            learn_rate=LEARN_RATE, \n",
    "            target_update_freq=10, \n",
    "            max_steps=MAX_STEPS,\n",
    "            show_agent=SHOW_AGENT\n",
    "        ))\n",
    "    )\n",
    "    reward_dist_freq_20.append(\n",
    "        np.mean(run_dqn(\n",
    "            env, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_episodes=NUM_EPISODES, \n",
    "            memory_size=MEMORY_SIZE, \n",
    "            num_hidden=NUM_HIDDEN, \n",
    "            discount_factor=DISCOUNT_FACTOR, \n",
    "            learn_rate=LEARN_RATE, \n",
    "            target_update_freq=10, \n",
    "            max_steps=MAX_STEPS,\n",
    "            show_agent=SHOW_AGENT\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "print(reward_dist_freq_10)\n",
    "print(reward_dist_freq_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can sometimes be a tricky question to decide whether one has collected enough scores to allow for meaningful comparisons, especially when this question has to be balanced against the cost of compute. When the variance in our scores is too high, we might be faced with misleading results, if it is sufficient, we run more models for no apparent reason. For this purpose, deepsig implements two different functions.\n",
    "\n",
    "First, we will take a look at bootstrap power analysis: It increases all scores in the sample by a certain factor, and then use bootstrapped versions of both samples and perform a significance test. Since the modified, new sample received a lift, the result should come out significant in most cases. If not, this is an indication that the original sample contains too much variance. Let's check that for our scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3576.79it/s]\n",
      " 15%|█████████████████████████████████▌                                                                                                                                                                                                    | 730/5000 [00:00<00:01, 3617.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3609.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(bootstrap_power_analysis(reward_dist_freq_10, seed=SEED))\n",
    "print(bootstrap_power_analysis(reward_dist_freq_20, seed=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores have a direct statistical interpretation, since they signify the *statistical power*. The higher the statistical power, the lower the probability of a Type II error or false negative, i.e. not rejecting the null hypothesis when it should be! A common rule of thumb is to thrive for a power of ~0.8, therefore we might want to collect more samples here. For instance, we could decide to collect 10 or 15 samples in total. \n",
    "\n",
    "Another, ASO-specific way to help us make that decision is the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n",
      "1.7320508075688772\n"
     ]
    }
   ],
   "source": [
    "print(aso_uncertainty_reduction(m_old=5, n_old=5, m_new=10, n_new=10))\n",
    "print(aso_uncertainty_reduction(m_old=5, n_old=5, m_new=15, n_new=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ASO only computes the \"true\" test score value in the limit of infinitely large samples, the estimate obtained using bootstrapping has some inherent variance, which can be reduced by adding more scores to the sample. The function above compute by what factor the uncertainty in the test result is being reduced. \n",
    "\n",
    "We can thus read the above as adding five more samples reducing the uncertainty by a factor of 1.41, while adding ten more sample only reduces it by 1.73. To strike a compromise with our computational budget, we thus only add five more samples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run #6...\n",
      "Performing run #7...\n",
      "Performing run #8...\n",
      "Performing run #9...\n",
      "Performing run #10...\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Performing run #{i+6}...\")\n",
    "    reward_dist_freq_10.append(\n",
    "        np.mean(run_dqn(\n",
    "            env, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_episodes=NUM_EPISODES, \n",
    "            memory_size=MEMORY_SIZE, \n",
    "            num_hidden=NUM_HIDDEN, \n",
    "            discount_factor=DISCOUNT_FACTOR, \n",
    "            learn_rate=LEARN_RATE, \n",
    "            target_update_freq=10, \n",
    "            max_steps=MAX_STEPS,\n",
    "            show_agent=SHOW_AGENT\n",
    "        ))\n",
    "    )\n",
    "    reward_dist_freq_20.append(\n",
    "        np.mean(run_dqn(\n",
    "            env, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_episodes=NUM_EPISODES, \n",
    "            memory_size=MEMORY_SIZE, \n",
    "            num_hidden=NUM_HIDDEN, \n",
    "            discount_factor=DISCOUNT_FACTOR, \n",
    "            learn_rate=LEARN_RATE, \n",
    "            target_update_freq=10, \n",
    "            max_steps=MAX_STEPS,\n",
    "            show_agent=SHOW_AGENT\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we repeat the bootstrap analysis again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3620.63it/s]\n",
      " 14%|████████████████████████████████▌                                                                                                                                                                                                     | 708/5000 [00:00<00:01, 3535.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3572.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(bootstrap_power_analysis(reward_dist_freq_10, seed=SEED))\n",
    "print(bootstrap_power_analysis(reward_dist_freq_20, seed=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power has increased! We now come back to the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 999/1000 [00:06<00:00, 165.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1315818429367487\n",
      "0.005\n"
     ]
    }
   ],
   "source": [
    "print(aso(rewards_freq_10, rewards_freq_20, num_jobs=4, seed=SEED))\n",
    "print(bootstrap_test(rewards_freq_10, rewards_freq_20, num_jobs=4, seed=SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can be fairly confident in our assessment! The last part of this demo wants to demonstrate how we could facilitate comparisons between multiple models at once, for which the package also implements a specific function. Let us first train a third kind of model for a number of runs. This time, we do not vary the update frequency of the target network, but instead the discount factor. Not that there is no specific reason we test eight runs here other than two demonstrate that ASO does not require equally-sized samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing run #1...\n",
      "Performing run #2...\n",
      "Performing run #3...\n",
      "Performing run #4...\n",
      "Performing run #5...\n",
      "Performing run #6...\n",
      "Performing run #7...\n",
      "Performing run #8...\n"
     ]
    }
   ],
   "source": [
    "reward_dist_discount_06 = []\n",
    "\n",
    "for i in range(8):\n",
    "    print(f\"Performing run #{i+1}...\")\n",
    "    reward_dist_discount_06.append(\n",
    "        np.mean(run_dqn(\n",
    "            env, \n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_episodes=NUM_EPISODES, \n",
    "            memory_size=MEMORY_SIZE, \n",
    "            num_hidden=NUM_HIDDEN, \n",
    "            discount_factor=0.6, \n",
    "            learn_rate=LEARN_RATE, \n",
    "            target_update_freq=10, \n",
    "            max_steps=MAX_STEPS,\n",
    "            show_agent=SHOW_AGENT\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model comparisons:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 2950/3000 [00:07<00:00, 387.04it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.25973275],\n",
       "       [1.        , 1.        , 0.1830454 ],\n",
       "       [1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_aso([reward_dist_freq_10, reward_dist_freq_20, reward_dist_discount_06], num_jobs=4, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read this result as the violation ratio of <row> compared  to <column> is value. Note that by suppling a dictionary as an argument and using `return_df=True`, we can output the result in a more readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model comparisons:   0%|                                                                                                                                                                                                                               | 0/3000 [00:00<?, ?it/s]\u001b[A\n",
      "Model comparisons:   2%|███▉                                                                                                                                                                                                                 | 56/3000 [00:00<00:07, 409.05it/s]\u001b[A\n",
      "Model comparisons:   4%|████████▍                                                                                                                                                                                                           | 120/3000 [00:00<00:06, 451.83it/s]\u001b[A\n",
      "Model comparisons:   8%|█████████████████▌                                                                                                                                                                                                  | 248/3000 [00:00<00:05, 473.94it/s]\u001b[A\n",
      "Model comparisons:  10%|██████████████████████                                                                                                                                                                                              | 312/3000 [00:00<00:06, 430.77it/s]\u001b[A\n",
      "Model comparisons:  13%|██████████████████████████▌                                                                                                                                                                                         | 376/3000 [00:00<00:06, 406.99it/s]\u001b[A\n",
      "Model comparisons:  15%|███████████████████████████████                                                                                                                                                                                     | 440/3000 [00:01<00:06, 396.16it/s]\u001b[A\n",
      "Model comparisons:  17%|███████████████████████████████████▌                                                                                                                                                                                | 504/3000 [00:01<00:06, 384.48it/s]\u001b[A\n",
      "Model comparisons:  19%|████████████████████████████████████████▏                                                                                                                                                                           | 568/3000 [00:01<00:06, 372.95it/s]\u001b[A\n",
      "Model comparisons:  21%|████████████████████████████████████████████▋                                                                                                                                                                       | 632/3000 [00:01<00:06, 369.37it/s]\u001b[A\n",
      "Model comparisons:  23%|█████████████████████████████████████████████████▏                                                                                                                                                                  | 696/3000 [00:01<00:06, 367.59it/s]\u001b[A\n",
      "Model comparisons:  25%|█████████████████████████████████████████████████████▋                                                                                                                                                              | 760/3000 [00:01<00:06, 366.36it/s]\u001b[A\n",
      "Model comparisons:  27%|██████████████████████████████████████████████████████████▏                                                                                                                                                         | 824/3000 [00:02<00:05, 364.29it/s]\u001b[A\n",
      "Model comparisons:  30%|██████████████████████████████████████████████████████████████▊                                                                                                                                                     | 888/3000 [00:02<00:05, 361.05it/s]\u001b[A\n",
      "Model comparisons:  32%|███████████████████████████████████████████████████████████████████▎                                                                                                                                                | 952/3000 [00:02<00:05, 357.11it/s]\u001b[A\n",
      "Model comparisons:  33%|██████████████████████████████████████████████████████████████████████▎                                                                                                                                            | 1000/3000 [00:02<00:08, 244.74it/s]\u001b[A\n",
      "Model comparisons:  35%|██████████████████████████████████████████████████████████████████████████▏                                                                                                                                        | 1055/3000 [00:03<00:06, 281.31it/s]\u001b[A\n",
      "Model comparisons:  37%|██████████████████████████████████████████████████████████████████████████████▋                                                                                                                                    | 1119/3000 [00:03<00:05, 321.26it/s]\u001b[A\n",
      "Model comparisons:  42%|███████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                           | 1247/3000 [00:03<00:04, 372.75it/s]\u001b[A\n",
      "Model comparisons:  44%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                      | 1311/3000 [00:03<00:04, 364.21it/s]\u001b[A\n",
      "Model comparisons:  46%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                  | 1375/3000 [00:03<00:04, 355.61it/s]\u001b[A\n",
      "Model comparisons:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 1439/3000 [00:03<00:04, 354.40it/s]\u001b[A\n",
      "Model comparisons:  50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                         | 1503/3000 [00:04<00:04, 355.43it/s]\u001b[A\n",
      "Model comparisons:  52%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                    | 1567/3000 [00:04<00:04, 351.43it/s]\u001b[A\n",
      "Model comparisons:  54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                | 1631/3000 [00:04<00:03, 349.53it/s]\u001b[A\n",
      "Model comparisons:  56%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 1695/3000 [00:04<00:03, 348.90it/s]\u001b[A\n",
      "Model comparisons:  59%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                       | 1759/3000 [00:04<00:03, 346.94it/s]\u001b[A\n",
      "Model comparisons:  61%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                  | 1823/3000 [00:05<00:03, 348.92it/s]\u001b[A\n",
      "Model comparisons:  63%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                              | 1887/3000 [00:05<00:03, 345.68it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model comparisons:  65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                         | 1951/3000 [00:05<00:03, 345.35it/s]\u001b[A\n",
      "Model comparisons:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                      | 1999/3000 [00:05<00:04, 247.28it/s]\u001b[A\n",
      "Model comparisons:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 2054/3000 [00:05<00:03, 283.82it/s]\u001b[A\n",
      "Model comparisons:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                              | 2118/3000 [00:06<00:02, 324.40it/s]\u001b[A\n",
      "Model comparisons:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                     | 2246/3000 [00:06<00:02, 376.96it/s]\u001b[A\n",
      "Model comparisons:  77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                | 2310/3000 [00:06<00:01, 370.86it/s]\u001b[A\n",
      "Model comparisons:  79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 2374/3000 [00:06<00:01, 367.02it/s]\u001b[A\n",
      "Model comparisons:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                       | 2438/3000 [00:06<00:01, 364.38it/s]\u001b[A\n",
      "Model comparisons:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                   | 2502/3000 [00:07<00:01, 363.95it/s]\u001b[A\n",
      "Model comparisons:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 2566/3000 [00:07<00:01, 362.10it/s]\u001b[A\n",
      "Model comparisons:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 2630/3000 [00:07<00:01, 357.99it/s]\u001b[A\n",
      "Model comparisons:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                     | 2694/3000 [00:07<00:00, 353.92it/s]\u001b[A\n",
      "Model comparisons:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 2758/3000 [00:07<00:00, 351.88it/s]\u001b[A\n",
      "Model comparisons:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 2822/3000 [00:08<00:00, 346.18it/s]\u001b[A\n",
      "Model comparisons:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 2886/3000 [00:08<00:00, 344.53it/s]\u001b[A\n",
      "Model comparisons:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 2950/3000 [00:08<00:00, 343.88it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "res_df = multi_aso(\n",
    "    {\n",
    "        \"update freq = 10\": reward_dist_freq_10, \n",
    "        \"update freq = 20\": reward_dist_freq_20, \n",
    "        \"discount factor = 0.6\": reward_dist_discount_06\n",
    "    },\n",
    "    num_jobs=4, seed=SEED, return_df=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>update freq = 10</th>\n",
       "      <th>update freq = 20</th>\n",
       "      <th>discount factor = 0.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>update freq = 10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.259733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update freq = 20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.183045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount factor = 0.6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       update freq = 10  update freq = 20  \\\n",
       "update freq = 10                    1.0               1.0   \n",
       "update freq = 20                    1.0               1.0   \n",
       "discount factor = 0.6               1.0               1.0   \n",
       "\n",
       "                       discount factor = 0.6  \n",
       "update freq = 10                    0.259733  \n",
       "update freq = 20                    0.183045  \n",
       "discount factor = 0.6               1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Model comparisons: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 2997/3000 [00:18<00:00, 387.04it/s]"
     ]
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can conclude here that lowering the discount factor actually seems to have a negative impact on the obtained rewards. \n",
    "\n",
    "First of all, thank for following this demo so far! Before letting you play with the different functions yourself, here a few disclaimers:\n",
    "\n",
    "1. This demo didn't try to put forth a realistic experimental pipeline in Reinforcement learning - the cart pole problem is just a cute problem for demonstration purposes.\n",
    "2. The use of significance threshold is very controversial, and ASO is no exception - instead of marking your results as significant / non-significant, report the output of the scores along with your effect size.\n",
    "3. Significance tests aren't perfect and come with a certain degree of uncertainty, and ASO is no exception\n",
    "    \n",
    "For more information on the functions, check out the documentation under https://deep-significance.readthedocs.io/en/latest/ or leave an issue on the Github repository https://github.com/Kaleidophon/deep-significance.\n",
    "\n",
    "\n",
    "### Bibliography\n",
    "\n",
    "* Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that cansolve difficult learning control problems.IEEE transactions on systems, man, and cybernetics, (5):834–846, 1983.\n",
    "\n",
    "* Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level controlthrough deep reinforcement learning.nature, 518(7540):529–533, 2015\n",
    "\n",
    "* Hado Van Hasselt,  Yotam Doron,  Florian Strub,  Matteo Hessel,  Nicolas Sonnerat,  and JosephModayil.  Deep reinforcement learning and the deadly triad.arXiv preprint arXiv:1812.02648,2018.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
